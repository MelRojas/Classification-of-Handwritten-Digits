<div class="step-text">
<p>In data preprocessing, we aim to transform raw data into a form that will help to increase the performance of a machine learning algorithm. Feature scaling is one of the techniques in data preprocessing, it's used to transform the independent features in a dataset so the model can interpret the features to the same degree. In today's topic, we will look at the most popular ways to perform feature scaling with the <code class="language-python">scikit-learn</code> package.</p><h5 id="initial-setup">Initial setup </h5><p>The <code class="language-python">scikit-learn</code> library comes with the <code class="language-python">sklearn.preprocessing</code> module, which provides utilities for feature scaling. We will be using the California housing dataset with 2 out of 8 features, available in the <code class="language-python">sklearn.datasets</code> module:</p><pre><code class="language-python">import pandas as pd
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing(as_frame=True)
df = data.frame[['MedInc', 'Population']]</code></pre><p>Here are the first few rows of our data:</p><pre><code class="language-no-highlight">+----+----------+--------------+
|    |   MedInc |   Population |
|----+----------+--------------|
|  0 |   8.3252 |          322 |
|  1 |   8.3014 |         2401 |
|  2 |   7.2574 |          496 |
+----+----------+--------------+</code></pre><p>Let's take a look at descriptive statistics for the two selected features by calling <code class="language-python">df.describe()</code>. <code class="language-python">MedInc</code> – median income in the block, <code class="language-python">Population</code> - block group population.</p><details><summary>Descriptive statistics</summary><div><pre><code class="language-no-highlight">+-------+-------------+--------------+
|       |      MedInc |   Population |
|-------+-------------+--------------|
| count | 20640       |     20640    |
| mean  |     3.87067 |      1425.48 |
| std   |     1.89982 |      1132.46 |
| min   |     0.4999  |         3    |
| 25%   |     2.5634  |       787    |
| 50%   |     3.5348  |      1166    |
| 75%   |     4.74325 |      1725    |
| max   |    15.0001  |     35682    |
+-------+-------------+--------------+</code></pre></div></details><p> The feature ranges look like this:</p><p><picture><img alt="The distribution of the unscaled features" height="480" src="https://ucarecdn.com/b8cf8e3d-2593-4d9e-b6f0-808d929f6c4d/" width="1248"/></picture></p><h5 id="a-note-on-fit_transform">A note on fit_transform()</h5><p>In this topic, we are using the <code class="language-python">.fit_transform()</code> method on the training data, which combines the <code class="language-python">.fit()</code> method that finds the parameters for a particular scaler and saves them as an internal object state, and <code class="language-python">.transform()</code>, which applies the parameters from the <code class="language-python">.fit()</code> to transform the data. To scale the test data, you only need to call <code class="language-python">.transform()</code> since the parameters are already present from applying <code class="language-python">.fit()</code> on the training data.</p><p style="text-align: center;"><picture><img alt=".fit(), .transform(), and .fit_transform() scheme for the sklearn estimator" height="507" src="https://ucarecdn.com/fceeb360-b23f-47a5-bc9a-71af03437cea/" width="800"/></picture></p><div class="alert alert-primary"><p>An important note: you should never scale the entire dataset before splitting it into the train and test sets. Scaling the entire data before the split may result in the leakage of the mean and variance from the test set into the training process. Always split the data before scaling, then use the statistics from the train split to scale the other splits.</p></div><h5 id="standardscaler">StandardScaler</h5><p>The <code class="language-python">StandardScaler</code> assumes the data is normally distributed within each feature and will scale them such that the distribution is now centered around 0 (with a mean, <span class="math-tex">\(\mu\)</span>, being 0), with a standard deviation (<span class="math-tex">\(\sigma\)</span>) of 1. The mean and standard deviation are calculated for the feature and then it is scaled. <code class="language-python">StandardScaler</code> doesn't change the distribution shape.</p><pre><code class="language-python">from sklearn.preprocessing import StandardScaler

scaler_std = StandardScaler()
df_standard = scaler_std.fit_transform(df)
df_standard = pd.DataFrame(df_standard, columns=df.columns)</code></pre><p style="text-align: center;"><picture><img alt="Distribution plots for before and after StandardScaler on 2 selected features" height="900" src="https://ucarecdn.com/07cde85a-3ef4-49c1-864b-cda0412dcc42/" width="1170"/></picture></p><p>We can see that now the mean is 0 and the standard deviation is 1 after standardization:</p><pre><code class="language-no-highlight">+----+------------+--------+-------+
|    | feature    |   mean |   std |
|----+------------+--------+-------|
|  0 | MedInc     |      0 |     1 |
|  1 | Population |      0 |     1 |
+----+------------+--------+-------+</code></pre><p>To name a few applications, standardization is a necessary preprocessing step in PCA. It's also applied in clustering since the feature comparison is based on distance measures.</p><h5 id="scaling-features-to-a-range">Scaling features to a range</h5><p></p><div class="alert alert-primary"><p>Feature scaling has some terminology ambiguities. In this topic, we refer to normalization as scaling features into the <span class="math-tex">\([0,1]\)</span> range with the <code class="language-python">MinMaxScaler</code> and the <code class="language-python">MaxAbsScaler</code> which operate on <em>features</em>, or <em>columns</em>. However, the term "normalization" is also used in the context of normalizing the <em>samples</em>, or the <em>rows, </em>with the <code class="language-python">Normalizer</code> class.</p></div><p></p><p>The <code class="language-python">MinMaxScaler</code> works by transforming the values into a specified range. By default, the <code class="language-python">MinMaxScaler</code> will rescale the data into the <span class="math-tex">\([0, 1]\)</span> range, but that can be changed by passing the required range to the <code class="language-python">feature_range</code> parameter. The <code class="language-python">MinMaxScaler</code> does what we previously defined as normalization. Both <code class="language-python">MinMaxScaler</code>and <code class="language-python">MaxAbsScaler</code> don't affect the distribution shape.</p><p>Below is an example of using the <code class="language-python">MinMaxScaler</code> class:</p><pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler

scaler_minmax = MinMaxScaler()
df_minmax = scaler_minmax.fit_transform(df)
df_minmax = pd.DataFrame(df_minmax, columns=df.columns)</code></pre><p>We can see that our output now ranges between 0 and 1.</p><pre><code class="language-no-highlight">+----+------------+-------+-------+
|    | feature    |   min |   max |
|----+------------+-------+-------|
|  0 | MedInc     |     0 |     1 |
|  1 | Population |     0 |     1 |
+----+------------+-------+-------+</code></pre><p style="text-align: center;"><picture><img alt="Distribution plots for before and after MinMaxScaler on 2 selected features" height="900" src="https://ucarecdn.com/3b6707f0-606c-4176-8c3f-02c96023139c/" width="1170"/></picture></p><p><code class="language-python">MaxAbsScaler</code> scales each feature by its maximum absolute value. It translates the feature individually such that the maximal absolute value of each feature in the training set will be 1.0.</p><pre><code class="language-python">from sklearn.preprocessing import MaxAbsScaler

scaler_maxabs = MaxAbsScaler()
df_maxabs = scaler_maxabs.fit_transform(df)
df_maxabs = pd.DataFrame(df_maxabs, columns=df.columns)</code></pre><table><tbody><tr></tr></tbody></table><p>The maximum value of transformed data is 1 and the minimum is 0 since the feature values here are strictly positive. If the data only had negative values it would be scaled to a minimum of -1.0 and a maximum of 0. If the dataset contained both negative and positive values the maximum value would be 1 and the minimum would be -1. <code class="language-python">MaxAbsScaler</code> is applied when the data is <em>sparse</em>, where a lot of the values don't contain data (such as <code class="language-python">NaN</code> values) — the <code class="language-python">MaxAbsScaler</code> doesn't shift or center and the sparsity is preserved. One of the possible applications is time series analysis.</p><h5 id="normalizer">Normalizer</h5><p>The <code class="language-python">Normalizer</code> rescales each sample (the row) <em>individually</em> for its norm (<span class="math-tex">\(L_1\)</span>, <span class="math-tex">\(L_2\)</span> or <span class="math-tex">\(\max\)</span>) to equal 1, with <span class="math-tex">\(L_2\)</span> being a default norm. <strong>The counter-intuitive part here is how the <em>rows</em>, and <em>not the columns</em>, are scaled to have a unit norm.</strong> After each element is squared and summed up, the total for the <span class="math-tex">\(L_2\)</span> norm will be 1. The data is squeezed between 0 and 1. <code class="language-python">Normalizer</code> changes the distribution shape.</p><pre><code class="language-python">from sklearn.preprocessing import Normalizer

normalizer = Normalizer()
df_norm = normalizer.fit_transform(df)
df_norm = pd.DataFrame(df_norm, columns=df.columns)</code></pre><p><code class="language-python">Normalizer</code> helps to avoid gradient explosion during training, since the features decrease in range and magnitude. It's also widely used in information retrieval and clustering.</p><h5 id="a-comparative-table-of-the-transformations">A comparative table of the transformations</h5><table align="center" border="1" cellpadding="1" cellspacing="1" style="width: 973px;"><tbody><tr><td><p>The transformation class in <code class="language-python">scikit-learn</code></p></td><td><p>The main takeaway</p></td><td><p>Does the distribution change?</p></td></tr><tr><td><p><code class="language-python">StandardScaler</code></p></td><td><p>Less sensitive to outliers, one of the most widely used transformations</p></td><td><p>No</p></td></tr><tr><td><p><code class="language-python">MinMaxScaler</code>,</p><p><code class="language-python">MaxAbsScaler</code></p></td><td><p>Suitable for data without outliers, transforms the features to lie in a certain range</p></td><td><p>No</p></td></tr><tr><td><p><code class="language-python">Normalizer</code></p></td><td><p>Rescales each sample individually to have a unit norm</p></td><td><p>Yes</p></td></tr></tbody></table><p> </p><h5 id="conclusion">Conclusion</h5><p>We have covered some of the most popular techniques for feature scaling using the <code class="language-python">scikit-learn</code> library. It is an important step of data preprocessing when dealing with machine learning algorithms such as:</p><ul><li><p>Distance-based algorithms (K-nearest neighbors, K-Means)</p></li><li><p>Clustering and principal component analysis (PCA)</p></li><li><p>Gradient-based algorithms, logistic regression, and SVMs.</p></li></ul><p>The rule-based algorithms are one of the few solutions that are not affected by scaling.</p>
</div>